{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059bceb6",
   "metadata": {},
   "source": [
    "<h1> Deep Q Network on Image Sequences to play game </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38915e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image  # To transform the image in the Processor\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Convolutional Backbone Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Keras-RL\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184af2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "nb_actions = env.action_space.n #action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc02a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (84, 84) #input image shape\n",
    "WINDOW_LENGTH = 4 #window length sequence in buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61cf88",
   "metadata": {},
   "source": [
    "Now we create the image processor. It is the same processor as in the preprocessing notebook, with the addition that it standardizes the data into the [0, 1] interval which often decreases the necessary training time. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d519049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        # First convert the numpy array to a PIL Image\n",
    "        img = Image.fromarray(observation)\n",
    "        # Then resize the image\n",
    "        img = img.resize(IMG_SHAPE)\n",
    "        # And convert it to grayscale  (The L stands for luminance)\n",
    "        img = img.convert(\"L\")\n",
    "        # Convert the image back to a numpy array and finally return the image\n",
    "        img = np.array(img)\n",
    "        return img.astype('uint8')  # saves storage in experience memory\n",
    "    \n",
    "    def process_state_batch(self, batch):\n",
    "\n",
    "        # We divide the observations by 255 to compress it into the intervall [0, 1].\n",
    "        # This supports the training of the network\n",
    "        # We perform this operation here to save memory.\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b0c611",
   "metadata": {},
   "source": [
    "As our input consists of 4 consecutive frames, each having the shape $(84 \\times 84)$, the input to the network has the shape $(84 \\times 84 \\times 4)$.\n",
    "But as the Convolutional Layers expect our input to be of shape $(4 \\times 84 \\times 84)$ , a permute layer is added at the beginning to swap the channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1d22e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH, IMG_SHAPE[0], IMG_SHAPE[1])\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3667d",
   "metadata": {},
   "source": [
    "Now it is time to define the network!\n",
    "We use the He Normal weight initialization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d936bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4),kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e3a30",
   "metadata": {},
   "source": [
    "Defining the sequentual memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a465d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9632693",
   "metadata": {},
   "source": [
    "Then we define the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7819ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ImageProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291c0ac",
   "metadata": {},
   "source": [
    "I have used the LinearAnnealedPolicy to implement the epsilon greedy action selection with decaying epsilon.\n",
    "\n",
    "As this network need to train for at least a million steps, I have set the number of steps to 1,000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0be40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2437a17",
   "metadata": {},
   "source": [
    "Defining and Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcb65474",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "              train_interval=4, delta_clip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0487bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b1967bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filename = 'dqn_breakout_weights.h5f'\n",
    "checkpoint_weights_filename = 'dqn_' + \"BreakoutDeterministic-v4\" + '_weights_{step}.h5f'\n",
    "checkpoint_callback = ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a60c84",
   "metadata": {},
   "source": [
    "If you do not want to waste time on initial trianing, **load_weights()** function provided by tensorflow. <br />\n",
    "\n",
    "Note that you would need to reduce to set a reduced epsilon if you are loading my pre-trained weights and start training from there.\n",
    "\n",
    "If you want to see the results and the performance of the DQN after 1.2 million episodes of training, go and run cell "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ffbcc",
   "metadata": {},
   "source": [
    "<h1> Run Below Cells if you want to train </h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16bf0887",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   10/10000 [..............................] - ETA: 1:00 - reward: 0.0000e+00 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahavir Dabas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  459/10000 [>.............................] - ETA: 55s - reward: 0.0109done, took 2.784 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a472bd1f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"weights/dqn_BreakoutDeterministic-v4_weights_900000.h5f\")\n",
    "\n",
    "# Update the policy to start with a smaller epsilon\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=0.3, value_min=.1, value_test=.05,\n",
    "                              nb_steps=100000)\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])\n",
    "\n",
    "# And train the model\n",
    "dqn.fit(env, nb_steps=500000, callbacks=[checkpoint_callback], log_interval=10000, visualize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdb049",
   "metadata": {},
   "source": [
    "<b> testing the trained model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76a4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahavir Dabas\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680b3a7",
   "metadata": {},
   "source": [
    "<b> testing the trained model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6bb5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=1500000, callbacks=[checkpoint_callback], log_interval=10000, visualize=False)\n",
    "\n",
    "# After training is done, we save the final weights one more time.\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7509719",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d008e04",
   "metadata": {},
   "source": [
    "<h1> Run these cells if you only want to evaluate performance on my weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"weights/dqn_BreakoutDeterministic-v4_weights_100000.h5f\")\n",
    "\n",
    "#You can chose an arbitrary policy for evaluation, it is fixed here.\n",
    "policy = EpsGreedyQPolicy(0.1)\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0b99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5627a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"weights/dqn_BreakoutDeterministic-v4_weights_600000.h5f\")\n",
    "\n",
    "#You can chose an arbitrary policy for evaluation, it is fixed here.\n",
    "policy = EpsGreedyQPolicy(0.1)\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99cc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028589f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04f6059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"weights/dqn_BreakoutDeterministic-v4_weights_1200000.h5f\")\n",
    "\n",
    "#You can chose an arbitrary policy for evaluation, it is fixed here.\n",
    "policy = EpsGreedyQPolicy(0.1)\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12518bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 40.000, steps: 1517\n",
      "Episode 2: reward: 40.000, steps: 1513\n",
      "Episode 3: reward: 40.000, steps: 1513\n",
      "Episode 4: reward: 40.000, steps: 1513\n",
      "Episode 5: reward: 40.000, steps: 1513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a4ec6aaf0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa5a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
