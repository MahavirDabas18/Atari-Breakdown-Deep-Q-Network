{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9768ccbd",
   "metadata": {},
   "source": [
    "<h1> Deep Q Network on Image Sequences to play game </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5adad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image  # To transform the image in the Processor\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Convolutional Backbone Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Keras-RL\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d052cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "nb_actions = env.action_space.n #action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc920484",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (84, 84) #input image shape\n",
    "WINDOW_LENGTH = 4 #window length sequence in buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa7afa",
   "metadata": {},
   "source": [
    "Now we create the image processor. It is the same processor as in the preprocessing notebook, with the addition that it standardizes the data into the [0, 1] interval which often decreases the necessary training time. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6f4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        # First convert the numpy array to a PIL Image\n",
    "        img = Image.fromarray(observation)\n",
    "        # Then resize the image\n",
    "        img = img.resize(IMG_SHAPE)\n",
    "        # And convert it to grayscale  (The L stands for luminance)\n",
    "        img = img.convert(\"L\")\n",
    "        # Convert the image back to a numpy array and finally return the image\n",
    "        img = np.array(img)\n",
    "        return img.astype('uint8')  # saves storage in experience memory\n",
    "    \n",
    "    def process_state_batch(self, batch):\n",
    "\n",
    "        # We divide the observations by 255 to compress it into the intervall [0, 1].\n",
    "        # This supports the training of the network\n",
    "        # We perform this operation here to save memory.\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da3c28",
   "metadata": {},
   "source": [
    "As our input consists of 4 consecutive frames, each having the shape $(84 \\times 84)$, the input to the network has the shape $(84 \\times 84 \\times 4)$.\n",
    "But as the Convolutional Layers expect our input to be of shape $(4 \\times 84 \\times 84)$ , a permute layer is added at the beginning to swap the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5e1bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH, IMG_SHAPE[0], IMG_SHAPE[1])\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7618152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4),kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2ed0b",
   "metadata": {},
   "source": [
    "Defining the sequentual memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "795fdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31513e",
   "metadata": {},
   "source": [
    "Then defining the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41bf4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ImageProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f34f03",
   "metadata": {},
   "source": [
    "I have used the LinearAnnealedPolicy to implement the epsilon greedy action selection with decaying epsilon.\n",
    "\n",
    "As this network need to train for at least a million steps, I have set the number of steps to 1,000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf9362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed737a",
   "metadata": {},
   "source": [
    "Defining and Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa9800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "              train_interval=4, delta_clip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77238226",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12c06a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_filename = 'dqn_breakout_weights.h5f'\n",
    "checkpoint_weights_filename = 'dqn_' + \"BreakoutDeterministic-v4\" + '_weights_{step}.h5f'\n",
    "checkpoint_callback = ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969197d",
   "metadata": {},
   "source": [
    "If you do not want to waste time on initial trianing, **load_weights()** function provided by tensorflow. <br />\n",
    "\n",
    "Note that you would need to reduce to set a reduced epsilon if you are loading my pre-trained weights and start training from there.\n",
    "\n",
    "If you want to see the results and the performance of the DQN after 1.2 million episodes, skip to the end of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1da47a",
   "metadata": {},
   "source": [
    "<h1> Run Below Cells if you want to train your own DQN </h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d48ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"weights/dqn_BreakoutDeterministic-v4_weights_900000.h5f\")\n",
    "\n",
    "# Update the policy to start with a smaller epsilon\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=0.3, value_min=.1, value_test=.05,\n",
    "                              nb_steps=100000)\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])\n",
    "\n",
    "# And train the model\n",
    "dqn.fit(env, nb_steps=500000, callbacks=[checkpoint_callback], log_interval=10000, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf3d1f",
   "metadata": {},
   "source": [
    "<b> testing the trained model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights one more time.\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd9326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "801535e1",
   "metadata": {},
   "source": [
    "<h1> Run these cells if you only want to evaluate performance on my weights </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b372fd",
   "metadata": {},
   "source": [
    "<b> weights after 100000 episodes of training </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74cbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"weights/dqn_BreakoutDeterministic-v4_weights_100000.h5f\")\n",
    "\n",
    "#You can chose an arbitrary policy for evaluation, it is fixed here.\n",
    "policy = EpsGreedyQPolicy(0.1)\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3183545",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2ca70",
   "metadata": {},
   "source": [
    "<b> weights after 1.1 million episodes of training </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"weights/dqn_BreakoutDeterministic-v4_weights_1100000.h5f\")\n",
    "\n",
    "#You can chose an arbitrary policy for evaluation, it is fixed here.\n",
    "policy = EpsGreedyQPolicy(0.1)\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9e41e",
   "metadata": {},
   "source": [
    "<b> weights after 1.2 million episodes of training </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"weights/dqn_BreakoutDeterministic-v4_weights_1200000.h5f\")\n",
    "\n",
    "#You can chose an arbitrary policy for evaluation, it is fixed here.\n",
    "policy = EpsGreedyQPolicy(0.1)\n",
    "\n",
    "\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
